{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection and Cleaning\n",
    "- Data was pulled from three txt files on Project Gutenberg. \n",
    "- The data pulled was unclean, with many return and new lines that needed to be removed. The text was cleaned and divided using the return line so each poem was classes as an individual string within a list. \n",
    "- Lastly, the poems were concatenated together for one corpus to be used in analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "edbook1 = urlopen(\"https://dev.gutenberg.org/files/2678/2678-0.txt\").read()\n",
    "edbook1 = BeautifulSoup(edbook1)\n",
    "edbook1 = edbook1.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "edbook1_ = edbook1.split('me!\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                ')\n",
    "bookpeoms = edbook1_[1]\n",
    "poems = bookpeoms.split('\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "poemtext1 = []\n",
    "for spaces in poems:\n",
    "    clean = spaces.replace('\\r\\n', ' ')\n",
    "    poemtext1.append(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "edbook2 = urlopen(\"https://dev.gutenberg.org/files/2679/2679.txt\").read()\n",
    "edbook2 = BeautifulSoup(edbook2)\n",
    "edbook2 = edbook2.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "edbook2_ = edbook2.split('\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n')\n",
    "bookpeoms = edbook2_[1]\n",
    "poems = bookpeoms.split('\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n        ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "poemtext2 = []\n",
    "for spaces in poems:\n",
    "    clean = spaces.replace('\\r\\n', ' ')\n",
    "    poemtext2.append(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "edbook3 = urlopen(\"https://dev.gutenberg.org/files/12241/12241.txt\").read()\n",
    "edbook3 = BeautifulSoup(edbook3)\n",
    "edbook3 = edbook3.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "edbook3_ = edbook3.split('_October_, 1896.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n')\n",
    "bookpeoms = edbook3_[1]\n",
    "poems = bookpeoms.split('\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "poemtext3 = []\n",
    "for spaces in poems:\n",
    "    clean = spaces.replace('\\r\\n', ' ')\n",
    "    poemtext3.append(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-1c7ec83459d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpoemtext3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m161\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoemtext3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop index out of range"
     ]
    }
   ],
   "source": [
    "poemtext3.pop(161)\n",
    "len(poemtext3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "emilyspoems = poemtext3 + poemtext2 + poemtext1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "- A baseline was conducted using the cleaned data before implementing stopwords and lemmatizing which are included below. \n",
    "\n",
    "### Removing Roman Numerials\n",
    "- There were a lot, they're all letters, also 'T is Tis T'is and every other combination of Tis was included in the Stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = open(\"stopwords.txt\", \"r\").read().split()  \n",
    "poems_stopped = []\n",
    "for item in emilyspoems:\n",
    "    tokens = nltk.word_tokenize(item)\n",
    "    stopped = ' '.join([word for word in tokens if word not in new_words])\n",
    "    poems_stopped.append(stopped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma=nltk.stem.WordNetLemmatizer()\n",
    "poems_lemmatized = []\n",
    "for item in poems_stopped:\n",
    "    tokens = nltk.word_tokenize(item)\n",
    "    stopped = ' '.join([lemma.lemmatize(token) for token in tokens])\n",
    "    poems_lemmatized.append(stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emilyspoems_noroman.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(poems_lemmatized, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "- Didn't yield any meaninful insights other than most poems were postive with low subjectivity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "feelings = []\n",
    "for item in emilyspoems:\n",
    "    feelings.append(TextBlob(item).sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
